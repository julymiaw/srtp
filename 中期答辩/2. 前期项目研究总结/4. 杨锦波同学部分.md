# Unixcoder（跨模态预训练模型）
## 一、模型概述
1.本模型支持与coding相关的代码理解和生成任务。

2.模型分类
(1)unixcoder-base-unimodal： 在 C4 和 CodeSearchNet 数据集（不含 NL）上进行预训练
(2)unixcoder-base： 继续在 CodeSearchNet 数据集的 NL-PL 对上预训练 unixcoder-base-unimodal。该模型可支持六种语言java、ruby、python、php、javascript 和 go。
(3)unixcoder-base-nine： 继续在 CodeSearchNet 数据集的 NL-PL 对和额外的 150 万个 C、C++ 和 C# 编程语言的 NL-PL 对上预训练。该模型可支持九种语言java、ruby、python、php、javascript、go、c、c++ 和 c#。

3.安装依赖
pytorch库，和transformers库
示例代码：
def _init_:构建 UniXcoder 模型的各个组件，并对一些属性进行初始化设置。

## 二、模型示例

以下部分的示例代码给出了不同模式下多个任务的零拍示例，包括代码搜索（仅编码器）、代码补全（仅解码器）、函数名称预测（编码器-解码器）、API 推荐（编码器-解码器）、代码总结（编码器-解码器）
（1）仅编码器
示例：代码搜索
对给定的函数字符串进行编码，并获取其对应的嵌入向量。
最大值函数，最小值函数，自然语言函数
```python
### Encode maximum function
func = "def f(a,b): if a>b: return a else return b"
tokens_ids = model.tokenize([func],max_length=512,mode="\<encoder-only>")
source_ids = torch.tensor(tokens_ids).to(device)
tokens_embeddings,max_func_embedding = model(source_ids)

### Encode minimum function
func = "def f(a,b): if a<b: return a else return b"
tokens_ids = model.tokenize([func],max_length=512,mode="\<encoder-only>")
source_ids = torch.tensor(tokens_ids).to(device)
tokens_embeddings,min_func_embedding = model(source_ids)

### Encode NL
nl = "return maximum value"
tokens_ids = model.tokenize([nl],max_length=512,mode="\<encoder-only>")
source_ids = torch.tensor(tokens_ids).to(device)
tokens_embeddings,nl_embedding = model(source_ids)

print(max_func_embedding.shape)
print(max_func_embedding)

#### the outcome of the print

torch.Size([1, 768])
tensor([[ 8.6533e-01, -1.9796e+00, -8.6849e-01,  4.2652e-01, -5.3696e-01,
         -1.5521e-01,  5.3770e-01,  3.4199e-01,  3.6305e-01, -3.9391e-01,
         -1.1816e+00,  2.6010e+00, -7.7133e-01,  1.8441e+00,  2.3645e+00,
				 ...,
         -2.9188e+00,  1.2555e+00, -1.9953e+00, -1.9795e+00,  1.7279e+00,
          6.4590e-01, -5.2769e-02,  2.4965e-01,  2.3962e-02,  5.9996e-02,
          2.5659e+00,  3.6533e+00,  2.0301e+00]], device='cuda:0',
       grad_fn=\<DivBackward0>)
```
现在，我们计算 NL （自然语言）和两个函数之间的余弦相似度（通过对嵌入向量归一化）。虽然两个函数的差值只是一个运算符（< 和 >），但 UniXcoder 可以将它们区分开来。
```python
### Normalize embedding
norm_max_func_embedding = torch.nn.functional.normalize(max_func_embedding, p=2, dim=1)
norm_min_func_embedding = torch.nn.functional.normalize(min_func_embedding, p=2, dim=1)
norm_nl_embedding = torch.nn.functional.normalize(nl_embedding, p=2, dim=1)

max_func_nl_similarity = torch.einsum("ac,bc->ab",norm_max_func_embedding,norm_nl_embedding)
min_func_nl_similarity = torch.einsum("ac,bc->ab",norm_min_func_embedding,norm_nl_embedding)

print(max_func_nl_similarity)
print(min_func_nl_similarity)

#### the outcome of the max_func_nl_similarity and min_func_nl_similarity

tensor([[0.3002]], device='cuda:0', grad_fn=\<ViewBackward>)
tensor([[0.1881]], device='cuda:0', grad_fn=\<ViewBackward>)
```
可以看到，最大值函数与nl函数的余弦相似度为0.3002，而最小值函数与nl函数的余弦相似度仅为0.1881
（2）仅解码器
示例：代码补全
```python
context = """
def f(data,file_path):
    # write json data into file_path in python language
"""
tokens_ids = model.tokenize([context],max_length=512,mode="\<decoder-only>")
source_ids = torch.tensor(tokens_ids).to(device)
prediction_ids = model.generate(source_ids, decoder_only=True, beam_size=3, max_length=128)
predictions = model.decode(prediction_ids)
print(context+predictions[0][0])

#### the outcome of the print

def f(data,file_path):
    # write json data into file_path in python language
    data = json.dumps(data)
    with open(file_path, 'w') as f:
        f.write(data)
```
通过给定的上下文文本利用模型做出预测，输出也是将data转换为json格式的字符串并写入对应文件路径里。
（3）解码-编码模式
示例：函数名称预测，api推荐，代码总结
首先，函数名被用 \<mask0> 代替，然后经过模型的编码和解码，做出预测之后，将predictions[0]返回到相应位置（以下为示例代码）
```python
context = """
def \<mask0>(data,file_path):
    data = json.dumps(data)
    with open(file_path, 'w') as f:
        f.write(data)
"""
tokens_ids = model.tokenize([context],max_length=512,mode="\<encoder-decoder>")
source_ids = torch.tensor(tokens_ids).to(device)
prediction_ids = model.generate(source_ids, decoder_only=False, beam_size=3, max_length=128)
predictions = model.decode(prediction_ids)
print([x.replace("\<mask0>","").strip() for x in predictions[0]])

#### the outcome of the print

['write_json', 'write_file', 'to_json']
```
结果符合预期的函数名称。
其次，函数体当中的data函数被 \<mask0> 代替，预测之后的结果即为可能的API，预测的函数为json的函数调用，包括dumps，loads
然后，函数体的注释被 \<mask0> 代替，预测之后的结果为“将json写入文件”类似英文内容。

##  三、预训练任务：（Fine-tune）
### 数据集下载
根据微软官方文档的提示，我们已经下载好了三个数据集，分别是Advtest（仅包含python代码的数据集），CSN（包含六种语言java、ruby、python、php、javascript 和 go的数据集）， CosQA （微软必应搜索引擎的 20,604 个搜索日志）。
### Zero-shot Setting
我们首先提供了零次代码搜索的脚本（即评估预训练之前的数据集）。本部分的源模型来自于microsoft/unixcoder-base，我们将直接在这个模型上进行test和evaluate。我们使用的代码和 nl 之间的相似度是 UniXcoder 隐藏状态的余弦距离。
零样本学习指的是在数据集没有经过训练之前的一次评估，我们可以根据训练前后的评估结果来对模型的预训练效果进行评测。
以Advtest数据集为例，我们可以看到对run.py文件中的一些参数设置：
```shell
python run.py \
    --output_dir saved_models/AdvTest \
    --model_name_or_path microsoft/unixcoder-base  \
    --do_zero_shot \
    --do_test \
    --test_data_file dataset/AdvTest/test.jsonl \
    --codebase_file dataset/AdvTest/test.jsonl \
    --num_train_epochs 2 \
    --code_length 256 \
    --nl_length 128 \
    --train_batch_size 64 \
    --eval_batch_size 64 \
    --learning_rate 2e-5 \
    --seed 123456
```
可以看到，在zero-shot设置中，仅有--do_test和--do_zero_shot两项，并不包含--do_train.
在pycharm上运行之后的结果如下：eval_mrr=0.431（评测满分为1分）。


### 预处理数据格式
这里以Advtest数据集为例，该数据集主要提供python语言的代码，数据格式如下：
```json
{   
    repo:"...",
    path:"...",
    func_name:"...",
    original_string:"...",
    language:"...",
    code:"...",
    code_tokens:"...",
    docstring:"...",
    docstring_tokens:"...",
    sha:"...",
    url:"..."
}
```