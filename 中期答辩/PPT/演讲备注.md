# 演讲备注

## 标题页

大家好，今天由我和张闻启同学来完成我们基于深度学习的源代码错误定位研究中期成果汇报

## 目录页

我们的汇报一共包含这6个部分

## 一、目标回顾

首先，让我们回顾一下项目任务书中提到的目标

(翻页)

以上是在项目计划申请书中给出的项目研究计划安排，截至目前，我们已经完成了前四个部分。

(翻页)

为了明确项目目标和创新点，我们阅读了来自4个期刊数据库的共100篇论文，仔细探索了大量基于机器学习的错误定位技术。我们最终选择基于预训练模型的静态错误定位作为技术路线。

(翻页)

在数据集选取部分，我们选择了6个Java开源项目，共包含超过22000个错误报告。

(翻页)

模型搭建部分，我们分工合作，先筛选出3种预训练NL-PL大模型，再分别阅读论文和模型源代码，

目前已在云服务器上运行了模型代码，并能够根据需要调整模型参数和输出。

## 二、成员分工

接下来，让我们回顾一下开题汇报时给出的成员分工。

(翻页)

这是开题答辩时给出的成员分工，因为预训练模型构建任务过于繁重，我们及时进行了调整。

(翻页)

现在，由张闻启同学负责构建数据集，其他同学把研究重心放在预训练大模型上。

(翻页)

此外，我们还增加了一名新成员，万奕含同学。

他有使用深度学习模型进行情感分析任务的经验，负责CodeBERT模型的构建。

## 三、数据集处理

下一个部分是数据集处理，由张闻启同学汇报

(下台)



(上台)

## 四、深度学习模型

接下来，继续由我来介绍深度学习模型的学习成果。

(翻页)

我们一共选择了3个模型，其中第一个模型已被其他研究用于错误定位任务，而另外两个模型在深度学习领域已被证明是更先进的。

(翻页)

CodeBERT模型的主要特点是，在RoBERTa模型的基础上，将输入从自然语言拓展到了自然语言和代码语言，将模型由NLP模型转变为NL-PL双峰模型。

(翻页)

在CodeBERT的基础上，GraphCodeBert选择用数据流图(Data Flow Graph)来进行代码预处理。

并针对图数据的特性，训练时在mask预训练任务的基础上增加了边预测任务。

(翻页)

接下来让我们通过一个简单的比大小函数，看看数据流图预处理后的结果。

(点击，等待动画展示)

可以看到，右侧的数据流图记录了每一个变量的数据流通关系。这些关系以有向图的形式作为额外信息被添加到注意力矩阵中。

(翻页)

接下来是第三个模型。UniXcoder模型采用了不一样的分词器，通过抽象语法树保留了结构信息

(翻页)

这是一段求最大值的Python函数，通过tree-sitter库提供的parser可以将其转换为抽象语法树结构

(翻页)

这是声明语句对应的树结构。接下来，来看看代码主体部分。

(翻页)

这是增加了第一行赋值语句后的抽象语法树

(点击，等待动画展示)

这是if分支语句

(翻页)

这是else分支语句

(翻页)

最后，是返回语句。

(点击，等待动画展示)

这三个模型均已在CodeSearchNet代码搜索数据集上进行预训练，但是错误定位任务与代码搜索任务有一定区别，例如一份错误报告往往对应多份错误代码。后续我们会对模型作进一步修改。

## 五、经费使用情况

费用主要源于云 GPU 的使用，以上是 3 月份东南大学云计算中心导出的费用清单， 

后续还需要进行大量模型训练与调试工作，这需要更多的经费投入。

(点击，等待动画展示)

在经费充足的情况下，我们会同时研究3个模型在错误定位领域的应用，否则我们可能会仅关注一个模型。

## 六、后期研究计划

在下一阶段，我们主要关注以下4个任务：

(点击一下，读一下标题)

## 结束页

感谢倾听，请各位评委老师批评指正！