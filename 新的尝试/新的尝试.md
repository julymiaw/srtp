# 待解决的问题

在此前研究中，将错误定位任务视为代码搜索任务，但代码搜索任务的训练数据是一对一的，即一个函数对应一个注释，例如：

```python
def get_vid_from_url(url):
	"""Extracts video ID from URL.
    """
	return match1(url, r'youtu\\.be/([^?/]+)') or
		   match1(url, r'youtube\\.com/embed/([^/?]+)') or
           match1(url, r'youtube\\.com/v/([^/?]+)') or
           match1(url, r'youtube\\.com/watch/([^/?]+)') or
           parse_query_param(url, 'v') or
           parse_query_param(parse_query_param(url, 'u'), 'v')
```

而错误报告往往一段长文本对应多个代码文件，例如TrackingBuggyFiles数据集中的报告“2edb246”中，描述为：

> ​	Discovered whilst working with Steve Ash on a build of a few projects that was consuming more than 2Gigs of heap.  
> ​	To recover memory Steve activated type demotion. This didn't appear to help much. This was due to Steve's projects using aspectpath. The aspectpath scanning to discover aspects was inadvertently making any types discovered on the aspectpath permanent types (not expendable) and they'd never be demoted/evicted.
> ​	The types were all being made permanent in case they were an aspect but never being demoted if it turns out they were not.  In a Roo petclinic I added spel as a dependency (on the aspectpath) and parsed a simple expression (just to further exaggerate the problem).  This was leaving 213 types in the fixed area of the typemap.  By correctly scanning aspectpath and demoting non-aspects this was reduced to 90.

而对应的更改有多个，其中一个是：

```
diff --git tests/multiIncremental/PR278496_10/base/com/Foo.java tests/multiIncremental/PR278496_10/base/com/Foo.java
new file mode 100644
index 0000000000..3b6c264d79
--- /dev/null
+++ tests/multiIncremental/PR278496_10/base/com/Foo.java
@@ -0,0 +1,7 @@
+package com;
+
+public class Foo {
+  String[] stringArray = new String[1];
+  int[] ints = new int[3];
+  float[][] floats = new float[1][2];
+}
",
```

在这个例子中，创建了一个新文件，并加入了7行新内容，而其他更改也是以创建新文件的方式。这些更改是彼此独立的，所以错误报告中的描述对每个文件都分别适用。因此，此前研究选择将每一个修改的文件分别与错误报告文本组合，一个错误报告生成多组训练样本。

但是，这种处理方式忽略了两种潜在的问题：

1. 修改原有文件而不是创建新文件

   在这种情况下，如果原有文件较大，整个文件与错误报告描述相关的部分占比极小，导致代码与描述的相关性较低。

2. 多个修改文件存在相关性

   在这种情况下，拆分成多个样本会损失修改文件间的联系信息，且每个文件与错误报告的相关性都较弱，例如当错误报告的前半段对应一个文件，后半段对应另一个文件时，每个文件与报告的相关性最高不超过50%。

CodeBERT和其衍生的模型为自然语言和代码语言分别训练一个编码器，并通过余弦相似度判断相关性强弱。相关性较弱的训练样本会带来噪声，使模型更容易将无关文件视为可疑文件。

此外，为了尽可能保存代码的语义结构信息，GraphCodeBert引入了数据流图（DFG），它通过TreeSitter库对函数生成抽象语法树，然后，通过递归调用一个针对不同节点类型的switch语句生成数据流图，其中每个节点与代码序列存在一一对应关系。CodeSearchNet数据集以函数为基本单位，函数长度较短，且包含相对完整的运行环境。因此，只有极少数函数超出了长度限制。

然而，在由git记录的错误提交日志中，往往是一系列文件中的语句更改。这种情况下，更改的语句本身难以形成一个完整的结构，很难为其生成数据流图或其他有效的图结构，而如果包含完整的文件，又更容易超出Bert的序列长度限制（一般为512）。

似乎只有当错误更改方式为创建一个新文件时，使用整个代码文件才是合理的。如果能将整个项目中与修改部分代码有关的部分提取出来，获取跨文件的数据流关系，那么就可以为一篇错误报告绘制一个完整的数据流图。问题是，现有的分析工具往往以函数为基本单位，我尝试使用基于字节码的“joern”工具，但它无法做到与源代码的一一对应，难以生成数据流图。
